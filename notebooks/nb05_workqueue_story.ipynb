{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee96fc2",
   "metadata": {},
   "source": [
    "# NB-05 - Workqueue Story (marts-only)\n",
    "\n",
    "## Purpose\n",
    "Generate a decision-ready workqueue snapshot and memo from marts-only sources.\n",
    "\n",
    "## Data Requirements\n",
    "- **DS3: required**\n",
    "- **DS0: optional** (only used if explicitly enabled)\n",
    "\n",
    "## Parameters\n",
    "- Top N: **25**\n",
    "- Sort: `at_risk_amt DESC`\n",
    "\n",
    "## Outputs\n",
    "- `workqueue_memo_latest_week.md` (ASCII-only)\n",
    "- `nb05_workqueue_top25.png`\n",
    "\n",
    "## How to Run\n",
    "1. Open this notebook\n",
    "2. Restart Kernel\n",
    "3. Run All\n",
    "4. Confirm outputs exist and are updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06b8e65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:05:07.573999Z",
     "iopub.status.busy": "2026-02-02T21:05:07.573738Z",
     "iopub.status.idle": "2026-02-02T21:05:08.479719Z",
     "shell.execute_reply": "2026-02-02T21:05:08.478996Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from notebooks.utils import story_blocks as sb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d98f45d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:05:08.481809Z",
     "iopub.status.busy": "2026-02-02T21:05:08.481522Z",
     "iopub.status.idle": "2026-02-02T21:05:08.484545Z",
     "shell.execute_reply": "2026-02-02T21:05:08.484033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "QUEUE_CAPACITY = 250\n",
    "TOP_N = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e04e97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:05:08.487056Z",
     "iopub.status.busy": "2026-02-02T21:05:08.486774Z",
     "iopub.status.idle": "2026-02-02T21:05:08.497428Z",
     "shell.execute_reply": "2026-02-02T21:05:08.496982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load marts only (DS3 required, DS0 optional)\n",
    "OFFLINE = os.getenv(\"OFFLINE\") == \"1\"\n",
    "\n",
    "BQ_PROJECT_ID = os.getenv(\"BQ_PROJECT_ID\") or os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
    "BQ_DATASET_ID = os.getenv(\"BQ_DATASET_ID\")\n",
    "\n",
    "# DS3 required (query top N only)\n",
    "DS3_TABLE = \"mart_workqueue_claims\"\n",
    "DS0_TABLE = \"mart_exec_overview_latest_week\"\n",
    "\n",
    "required_cols = [\n",
    "    \"desynpuf_id\",\n",
    "    \"clm_id\",\n",
    "    \"at_risk_amt\",\n",
    "    \"denied_potential_allowed_proxy_amt\",\n",
    "    \"payer_allowed_amt\",\n",
    "    \"observed_paid_amt\",\n",
    "    \"payer_yield_gap_amt\",\n",
    "    \"aging_days\",\n",
    "    \"p_denial\",\n",
    "    \"top_denial_group\",\n",
    "    \"top_next_best_action\",\n",
    "    \"top_hcpcs\",\n",
    "    \"top_denial_prcsg\",\n",
    "]\n",
    "\n",
    "select_cols = \", \".join(required_cols)\n",
    "\n",
    "def load_ds3():\n",
    "    if not BQ_PROJECT_ID or not BQ_DATASET_ID:\n",
    "        raise RuntimeError(\"BigQuery not configured. Set BQ_PROJECT_ID and BQ_DATASET_ID.\")\n",
    "    import pandas_gbq\n",
    "    query_ds3 = f\"\"\"\n",
    "SELECT {select_cols}\n",
    "FROM `{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{DS3_TABLE}`\n",
    "ORDER BY at_risk_amt DESC\n",
    "LIMIT {TOP_N}\n",
    "\"\"\"\n",
    "    return pandas_gbq.read_gbq(query_ds3, project_id=BQ_PROJECT_ID)\n",
    "\n",
    "def load_ds0():\n",
    "    if not BQ_PROJECT_ID or not BQ_DATASET_ID:\n",
    "        return pd.DataFrame()\n",
    "    import pandas_gbq\n",
    "    query_ds0 = f\"SELECT week_start, mix_stability_flag FROM `{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{DS0_TABLE}` LIMIT 1\"\n",
    "    return pandas_gbq.read_gbq(query_ds0, project_id=BQ_PROJECT_ID)\n",
    "\n",
    "ds3 = sb.load_table(\"ds3_top25\", OFFLINE, load_ds3, \"docs/fixtures/ds3_top25.csv\")\n",
    "ds0 = sb.load_table(\"ds0\", OFFLINE, load_ds0, \"docs/fixtures/ds0.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d813ecd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:05:08.499269Z",
     "iopub.status.busy": "2026-02-02T21:05:08.499064Z",
     "iopub.status.idle": "2026-02-02T21:05:08.504426Z",
     "shell.execute_reply": "2026-02-02T21:05:08.504005Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Guardrails ---\n",
    "REQUIRED_COLS = [\n",
    "    \"desynpuf_id\",\n",
    "    \"clm_id\",\n",
    "    \"at_risk_amt\",\n",
    "    \"denied_potential_allowed_proxy_amt\",\n",
    "    \"payer_allowed_amt\",\n",
    "    \"observed_paid_amt\",\n",
    "    \"payer_yield_gap_amt\",\n",
    "    \"aging_days\",\n",
    "    \"p_denial\",\n",
    "    \"top_denial_group\",\n",
    "    \"top_next_best_action\",\n",
    "    \"top_hcpcs\",\n",
    "    \"top_denial_prcsg\",\n",
    "]\n",
    "\n",
    "assert ds3 is not None, \"DF is None ? DS3 query did not return data.\"\n",
    "missing = [c for c in REQUIRED_COLS if c not in ds3.columns]\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "assert len(ds3) <= TOP_N, f\"Expected <= {TOP_N} rows, got {len(ds3)}\"\n",
    "\n",
    "# Ensure sort order\n",
    "if len(ds3) > 1:\n",
    "    assert (ds3[\"at_risk_amt\"].fillna(0).values[:-1] >= ds3[\"at_risk_amt\"].fillna(0).values[1:]).all(),         \"Expected df sorted by at_risk_amt DESC\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81aebbcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:05:08.506516Z",
     "iopub.status.busy": "2026-02-02T21:05:08.506301Z",
     "iopub.status.idle": "2026-02-02T21:05:08.511556Z",
     "shell.execute_reply": "2026-02-02T21:05:08.510997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Total at-risk sum for share calculation\n",
    "if OFFLINE:\n",
    "    if \"total_at_risk_amt\" in ds3.columns and len(ds3) > 0:\n",
    "        total_at_risk = float(ds3[\"total_at_risk_amt\"].iloc[0])\n",
    "    elif \"at_risk_amt\" in ds3.columns:\n",
    "        total_at_risk = float(ds3[\"at_risk_amt\"].fillna(0).sum())\n",
    "    else:\n",
    "        total_at_risk = 0.0\n",
    "else:\n",
    "    if not BQ_PROJECT_ID or not BQ_DATASET_ID:\n",
    "        raise RuntimeError(\"BigQuery not configured. Set BQ_PROJECT_ID and BQ_DATASET_ID.\")\n",
    "    import pandas_gbq\n",
    "    query_total = (\n",
    "        \"SELECT SUM(at_risk_amt) AS total_at_risk_amt \"\n",
    "        f\"FROM `{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{DS3_TABLE}`\"\n",
    "    )\n",
    "    total_df = pandas_gbq.read_gbq(query_total, project_id=BQ_PROJECT_ID)\n",
    "    if total_df.empty or total_df.iloc[0][\"total_at_risk_amt\"] is None:\n",
    "        total_at_risk = 0.0\n",
    "    else:\n",
    "        total_at_risk = float(total_df.iloc[0][\"total_at_risk_amt\"])\n",
    "\n",
    "# Top25 sum from current df\n",
    "if \"at_risk_amt\" in ds3.columns:\n",
    "    top25_sum = float(ds3[\"at_risk_amt\"].fillna(0).sum())\n",
    "else:\n",
    "    top25_sum = 0.0\n",
    "\n",
    "share_pct = (top25_sum / total_at_risk * 100) if total_at_risk else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b6d53b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:05:08.513645Z",
     "iopub.status.busy": "2026-02-02T21:05:08.513445Z",
     "iopub.status.idle": "2026-02-02T21:05:08.517512Z",
     "shell.execute_reply": "2026-02-02T21:05:08.517011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select ranking metric\n",
    "ranking_metric_used = \"at_risk_amt\" if \"at_risk_amt\" in ds3.columns else None\n",
    "if ranking_metric_used is None:\n",
    "    raise ValueError(\"DS3 missing ranking metric at_risk_amt\")\n",
    "\n",
    "# Sort and select top N\n",
    "queue_df = ds3.sort_values(ranking_metric_used, ascending=False).head(TOP_N).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529eae1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:05:08.520210Z",
     "iopub.status.busy": "2026-02-02T21:05:08.519916Z",
     "iopub.status.idle": "2026-02-02T21:05:08.525934Z",
     "shell.execute_reply": "2026-02-02T21:05:08.525476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build display columns (omit missing, note omissions)\n",
    "required_order = [\n",
    "    \"desynpuf_id\",\n",
    "    \"clm_id\",\n",
    "    \"at_risk_amt\",\n",
    "    \"denied_potential_allowed_proxy_amt\",\n",
    "    \"payer_allowed_amt\",\n",
    "    \"observed_paid_amt\",\n",
    "    \"payer_yield_gap_amt\",\n",
    "    \"aging_days\",\n",
    "    \"p_denial\",\n",
    "    \"top_denial_group\",\n",
    "    \"top_next_best_action\",\n",
    "    \"top_hcpcs\",\n",
    "    \"top_denial_prcsg\",\n",
    "]\n",
    "\n",
    "present_cols = [c for c in required_order if c in queue_df.columns]\n",
    "missing_cols = [c for c in required_order if c not in queue_df.columns]\n",
    "\n",
    "# Explainability column from marts fields only\n",
    "explain_parts = []\n",
    "for col in [\"top_denial_group\", \"top_next_best_action\", \"top_hcpcs\", \"top_denial_prcsg\"]:\n",
    "    if col in queue_df.columns:\n",
    "        explain_parts.append(col)\n",
    "\n",
    "if explain_parts:\n",
    "    def build_explain(row):\n",
    "        parts = []\n",
    "        if \"top_denial_group\" in explain_parts and pd.notna(row.get(\"top_denial_group\")):\n",
    "            parts.append(str(row.get(\"top_denial_group\")))\n",
    "        if \"top_next_best_action\" in explain_parts and pd.notna(row.get(\"top_next_best_action\")):\n",
    "            parts.append(str(row.get(\"top_next_best_action\")))\n",
    "        if \"top_hcpcs\" in explain_parts and pd.notna(row.get(\"top_hcpcs\")):\n",
    "            parts.append(\"HCPCS \" + str(row.get(\"top_hcpcs\")))\n",
    "        if \"top_denial_prcsg\" in explain_parts and pd.notna(row.get(\"top_denial_prcsg\")):\n",
    "            parts.append(\"PRCSG \" + str(row.get(\"top_denial_prcsg\")))\n",
    "        return \" | \".join(parts) if parts else \"\"\n",
    "\n",
    "    queue_df[\"Explain\"] = queue_df.apply(build_explain, axis=1)\n",
    "    present_cols = present_cols + [\"Explain\"]\n",
    "else:\n",
    "    missing_cols.append(\"Explain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6407ff5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:05:08.528042Z",
     "iopub.status.busy": "2026-02-02T21:05:08.527762Z",
     "iopub.status.idle": "2026-02-02T21:05:08.534504Z",
     "shell.execute_reply": "2026-02-02T21:05:08.533982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Format for display\n",
    "fmt_df = queue_df[present_cols].copy()\n",
    "\n",
    "money_cols = [\n",
    "    \"at_risk_amt\",\n",
    "    \"denied_potential_allowed_proxy_amt\",\n",
    "    \"payer_allowed_amt\",\n",
    "    \"observed_paid_amt\",\n",
    "    \"payer_yield_gap_amt\",\n",
    "]\n",
    "rate_cols = [\"p_denial\"]\n",
    "\n",
    "for col in money_cols:\n",
    "    if col in fmt_df.columns:\n",
    "        fmt_df[col] = fmt_df[col].apply(lambda v: f\"${float(v):,.0f}\" if pd.notna(v) else \"\")\n",
    "\n",
    "for col in rate_cols:\n",
    "    if col in fmt_df.columns:\n",
    "        fmt_df[col] = fmt_df[col].apply(lambda v: f\"{float(v)*100:,.1f}%\" if pd.notna(v) else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "525d25dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:05:08.536944Z",
     "iopub.status.busy": "2026-02-02T21:05:08.536680Z",
     "iopub.status.idle": "2026-02-02T21:05:08.729173Z",
     "shell.execute_reply": "2026-02-02T21:05:08.728522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote ..\\docs\\images\\nb05_workqueue_top25.png\n"
     ]
    }
   ],
   "source": [
    "# Render table to PNG (fallback to CSV if failure)\n",
    "img_dir = Path(\"docs\") / \"images\" if Path(\"docs\").is_dir() else Path(\"..\") / \"docs\" / \"images\"\n",
    "img_dir.mkdir(parents=True, exist_ok=True)\n",
    "img_path = img_dir / \"nb05_workqueue_top25.png\"\n",
    "\n",
    "csv_path = Path(\"docs\") / \"workqueue_top25.csv\" if Path(\"docs\").is_dir() else Path(\"..\") / \"docs\" / \"workqueue_top25.csv\"\n",
    "\n",
    "png_written = False\n",
    "try:\n",
    "    img_cols = [\n",
    "        'desynpuf_id',\n",
    "        'clm_id',\n",
    "        'at_risk_amt',\n",
    "        'denied_potential_allowed_proxy_amt',\n",
    "        'payer_allowed_amt',\n",
    "        'observed_paid_amt',\n",
    "        'p_denial',\n",
    "        'top_denial_group',\n",
    "    ]\n",
    "    img_cols = [c for c in img_cols if c in fmt_df.columns]\n",
    "    img_df = fmt_df[img_cols].copy() if img_cols else fmt_df.copy()\n",
    "\n",
    "    label_map = {\n",
    "        'desynpuf_id': 'id',\n",
    "        'clm_id': 'claim',\n",
    "        'at_risk_amt': 'at_risk',\n",
    "        'denied_potential_allowed_proxy_amt': 'denied_proxy',\n",
    "        'payer_allowed_amt': 'allowed',\n",
    "        'observed_paid_amt': 'paid',\n",
    "        'p_denial': 'p_denial',\n",
    "        'top_denial_group': 'denial_group',\n",
    "    }\n",
    "    col_labels = [label_map.get(c, c) for c in img_df.columns]\n",
    "\n",
    "    fig_height = max(6, 0.3 * (len(img_df) + 1))\n",
    "    fig, ax = plt.subplots(figsize=(14, fig_height))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(\n",
    "        cellText=img_df.values.tolist(),\n",
    "        colLabels=col_labels,\n",
    "        loc='center'\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 1.3)\n",
    "    fig.savefig(img_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    png_written = True\n",
    "    print(f\"Wrote {img_path}\")\n",
    "except Exception as exc:\n",
    "    png_written = False\n",
    "    fmt_df.to_csv(csv_path, index=False)\n",
    "    print(f\"PNG render failed; wrote {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab56e20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:05:08.731449Z",
     "iopub.status.busy": "2026-02-02T21:05:08.731229Z",
     "iopub.status.idle": "2026-02-02T21:05:08.742680Z",
     "shell.execute_reply": "2026-02-02T21:05:08.741982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote ..\\docs\\workqueue_memo_latest_week.md\n",
      "Wrote ..\\docs\\story\\nb05_workqueue.md\n"
     ]
    }
   ],
   "source": [
    "# Memo export (ASCII-only)\n",
    "now = datetime.now()\n",
    "\n",
    "anchor_week = \"N/A\"\n",
    "if not ds0.empty and \"week_start\" in ds0.columns:\n",
    "    try:\n",
    "        anchor_week = str(ds0[\"week_start\"].iloc[0])\n",
    "    except Exception:\n",
    "        anchor_week = \"N/A\"\n",
    "\n",
    "fields_omitted_note = \"\"\n",
    "if missing_cols:\n",
    "    fields_omitted_note = \"- Fields omitted: \" + \", \".join(missing_cols)\n",
    "\n",
    "def fmt_dollars0(x: float) -> str:\n",
    "    try:\n",
    "        return f\"${float(x):,.0f}\"\n",
    "    except Exception:\n",
    "        return \"N/A\"\n",
    "\n",
    "share = (top25_sum / total_at_risk) if total_at_risk else 0.0\n",
    "share_pct = f\"{share*100:.2f}%\"\n",
    "\n",
    "top25_line = f\"Top 25 at-risk: {fmt_dollars0(top25_sum)} ({share_pct} of total {fmt_dollars0(total_at_risk)})\"\n",
    "\n",
    "memo_lines = [\n",
    "    \"# Workqueue Memo - Prioritized Claims (Demo)\",\n",
    "    \"\",\n",
    "    \"## What this is\",\n",
    "    \"This is a prioritization demo built from DS3 marts outputs. It is not a guarantee of recoveries.\",\n",
    "    \"\",\n",
    "    \"## Receipt\",\n",
    "    f\"- Anchor week (if DS0 available): {anchor_week}\",\n",
    "    f\"- Ranking basis: {ranking_metric_used}\",\n",
    "    f\"- Top N shown: {TOP_N}\",\n",
    "    f\"- Capacity assumption: {QUEUE_CAPACITY}/day\",\n",
    "    \"- Source: mart_workqueue_claims (DS3)\",\n",
    "]\n",
    "\n",
    "gen_line = sb.generated_on_line(now)\n",
    "if gen_line:\n",
    "    memo_lines.append(f\"- {gen_line}\")\n",
    "\n",
    "memo_lines += [\n",
    "    \"\",\n",
    "    \"## Queue preview (Top N)\",\n",
    "    top25_line,\n",
    "]\n",
    "\n",
    "if png_written:\n",
    "    memo_lines.append(\"![Workqueue Top N](images/nb05_workqueue_top25.png)\")\n",
    "else:\n",
    "    memo_lines.append(\"See workqueue_top25.csv\")\n",
    "\n",
    "memo_lines += [\n",
    "    \"\",\n",
    "    \"## How to use this\",\n",
    "    \"- Start with the highest at-risk exposure.\",\n",
    "    \"- Use the explainability columns to route to the correct workflow (action/category).\",\n",
    "    \"- If mix stability is CHECK SEGMENTS in NB-03, treat queue as provisional until drivers are validated.\",\n",
    "    \"\",\n",
    "    \"## Guardrails\",\n",
    "    \"- Proxy values are directional prioritization only; not guaranteed recovery.\",\n",
    "    \"- Paid vs Allowed differences are claim-file amounts used for directional triage, not adjudicated underpayment findings.\",\n",
    "]\n",
    "\n",
    "if fields_omitted_note:\n",
    "    memo_lines.append(\"\")\n",
    "    memo_lines.append(fields_omitted_note)\n",
    "\n",
    "memo_lines = [line.encode(\"ascii\", \"ignore\").decode(\"ascii\") for line in memo_lines]\n",
    "\n",
    "memo_path = Path(\"docs\") / \"workqueue_memo_latest_week.md\" if Path(\"docs\").is_dir() else Path(\"..\") / \"docs\" / \"workqueue_memo_latest_week.md\"\n",
    "with open(memo_path, \"w\", encoding=\"utf-8\") as handle:\n",
    "    handle.write(\"\\n\".join(memo_lines))\n",
    "\n",
    "print(f\"Wrote {memo_path}\")\n",
    "\n",
    "# Story export (ASCII-only)\n",
    "story_dir = Path('docs') / 'story' if Path('docs').is_dir() else Path('..') / 'docs' / 'story'\n",
    "story_dir.mkdir(parents=True, exist_ok=True)\n",
    "story_path = story_dir / 'nb05_workqueue.md'\n",
    "\n",
    "capacity_list = [5, 10, 20]\n",
    "capacity_table = [\n",
    "    \"| Capacity (items/day) | Days for Top-25 |\",\n",
    "    \"| --- | --- |\",\n",
    "]\n",
    "for cap in capacity_list:\n",
    "    days = TOP_N / float(cap)\n",
    "    capacity_table.append(f\"| {cap} | {days:.2f} days |\")\n",
    "\n",
    "story_lines = [\n",
    "    '# Workqueue (NB-05) - Top 25 prioritized items (marts-only)',\n",
    "    '',\n",
    "    '## What this is',\n",
    "    'Operational prioritization demo using proxy ranking (not guaranteed recovery).',\n",
    "    '',\n",
    "    '## Visual',\n",
    "    '![Top 25 Workqueue](../images/nb05_workqueue_top25.png)',\n",
    "    '',\n",
    "    '## Context',\n",
    "    top25_line,\n",
    "    '',\n",
    "    '## Capacity framing',\n",
    "    *capacity_table,\n",
    "    '',\n",
    "    'Decision: pick a capacity target this week.',\n",
    "    'Execution: work items per day and track backlog burn against the Top-25 list.',\n",
    "    '',\n",
    "    '## Guardrails',\n",
    "    'Proxy ranking is directional only; not guaranteed recovery.',\n",
    "    '',\n",
    "    '## Next step',\n",
    "    'If INVESTIGATE: resolve mix/volume shift drivers first (NB-04), then allocate queue capacity.',\n",
    "]\n",
    "\n",
    "story_lines = [line.encode('ascii', 'ignore').decode('ascii') for line in story_lines]\n",
    "\n",
    "with open(story_path, 'w', encoding='utf-8') as handle:\n",
    "    handle.write('\\n'.join(story_lines))\n",
    "\n",
    "print(f'Wrote {story_path}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
